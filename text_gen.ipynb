{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6F3iHPL6Sma7"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, GRU, Dropout\n",
    "from tensorflow.keras import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for RNN was heavily influenced by Natural Language Generation course on Datacamp.com, we played around with the model \n",
    "This code was run for each genres corpus in google colab, i will just show one for brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Gx2hw2f1Sv1y",
    "outputId": "a908e4b8-f8a3-4370-e1d5-40dc7b7c3423",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'drive/My Drive/rap_corpus.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7551abaad023>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read in corpus from specific genre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'drive/My Drive/rap_corpus.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/My Drive/rap_corpus.txt'"
     ]
    }
   ],
   "source": [
    "# Read in corpus from specific genre\n",
    "text = open('drive/My Drive/rap_corpus.txt', 'rb').read().decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lqh-0y3iS6UC",
    "outputId": "0c1ab5cf-e393-4a6a-a9eb-ffcdeb6fc0fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 unique characters\n"
     ]
    }
   ],
   "source": [
    "# find unique characters in corpus\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "riWjz2afS8hD"
   },
   "outputs": [],
   "source": [
    "# create dictionary that maps a character to an integer\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "ztJmfSESS-wy",
    "outputId": "0fb50f83-560f-4a11-81df-16d3f7eadd21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\t':   0,\n",
      "  '\\n':   1,\n",
      "  ' ' :   2,\n",
      "  'a' :   3,\n",
      "  'b' :   4,\n",
      "  'c' :   5,\n",
      "  'd' :   6,\n",
      "  'e' :   7,\n",
      "  'f' :   8,\n",
      "  'g' :   9,\n",
      "  'h' :  10,\n",
      "  'i' :  11,\n",
      "  'j' :  12,\n",
      "  'k' :  13,\n",
      "  'l' :  14,\n",
      "  'm' :  15,\n",
      "  'n' :  16,\n",
      "  'o' :  17,\n",
      "  'p' :  18,\n",
      "  'q' :  19,\n",
      "  'r' :  20,\n",
      "  's' :  21,\n",
      "  't' :  22,\n",
      "  'u' :  23,\n",
      "  'v' :  24,\n",
      "  'w' :  25,\n",
      "  'x' :  26,\n",
      "  'y' :  27,\n",
      "  'z' :  28,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# show the mappings of the characters\n",
    "print('{')\n",
    "for char,_ in zip(char2idx, range(46)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uvb9SWIwTDVU",
    "outputId": "f6cebbdd-547a-4d29-9f11-b6f4df78d5af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'fill me up wi' ---- characters mapped to int ---- > [ 8 11 14 14  2 15  7  2 23 18  2 25 11]\n"
     ]
    }
   ],
   "source": [
    "# example of how characters are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "lRSMUjh7TFen",
    "outputId": "158a6d1a-a6a9-4c11-8d56-5f130383d4e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n",
      "i\n",
      "l\n",
      "l\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "3hsdJ4MkTHkk",
    "outputId": "c2578918-dae8-4999-fece-01d4b6513f1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'fill me up with false hope \\n cause i wish the world that i wasnt me \\n with no direction at all \\n im l'\n",
      "'osing faith in everything \\n \\n by my alone time \\n i can see for the first time \\n hurtfull words wont g'\n",
      "'o away \\n \\n i watch my dreams die off \\n it hurts to believe that words are just words \\n \\n dwelling on '\n",
      "'my own thoughts \\n choking on self proclaimed asperation \\n circumvent my own faults \\n for shadows coll'\n",
      "'apse in my heart \\n \\n reckling through my sunked life \\n shift a flame to all the pain \\n distant feelin'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CceUIhEzTKTI"
   },
   "outputs": [],
   "source": [
    "# creates our dataset that we will fit our model to\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6K9IWy0lTRBs",
    "outputId": "bea38c8a-db6c-4d13-ebc9-1c8f1fa4fe61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rwyq0Q0zTTMt"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJEppaIuTVbI"
   },
   "outputs": [],
   "source": [
    "# model that we will be building, starts with an embedding layer, then goes to two LSTM layers and finally a\n",
    "# dense layer the size of our vocab\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.LSTM(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        dropout = 0.15,\n",
    "                        recurrent_dropout = 0.15),\n",
    "    tf.keras.layers.LSTM(rnn_units, \n",
    "                         return_sequences=True,\n",
    "                         stateful=True,\n",
    "                         dropout = 0.15,\n",
    "                         recurrent_dropout = 0.15),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "UGStz2-NTadX",
    "outputId": "fd1830f5-5032-44d6-ba91-513babe32fc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "# build our model based on parameters we set earlier, we settled on these parameters as it gave us the best\n",
    "# results while also allowing us to fit all of our models in time\n",
    "model = build_model(\n",
    "    vocab_size = len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "zFWySfJgTgx3",
    "outputId": "fb571b7c-8aee-4160-de20-fbe4c0502bbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           7424      \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (64, None, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (64, None, 512)           2099200   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 29)            14877     \n",
      "=================================================================\n",
      "Total params: 3,696,413\n",
      "Trainable params: 3,696,413\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# summary of our model, we have just under 4 million parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "DmkNiyRZTuuD",
    "outputId": "a8e89167-846d-4682-fcd3-fab7635e154f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 29)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       3.3672702\n"
     ]
    }
   ],
   "source": [
    "# loss function taken from tensorflow documentation\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ppLqher0TxA-"
   },
   "outputs": [],
   "source": [
    "# compile our model with adam optimizer, the loss function, and showing accuracy in each epoch\n",
    "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KgEMT0NRTzCQ"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved (pro tip: when working in google colab save models to your\n",
    "# drive and not the session storage or you may lose all of your work)\n",
    "checkpoint_dir = 'drive/My Drive/rap'\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wCdw4i4_T1U_"
   },
   "outputs": [],
   "source": [
    "# number of epochs to run, running ten epochs took around 3 to 4 hours\n",
    "EPOCHS=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "yroNGt6KT3S5",
    "outputId": "694c1e5c-45ca-4bd0-d130-2f9b6e34c8b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6870/6870 [==============================] - 4498s 655ms/step - loss: 1.3897 - accuracy: 0.5742\n",
      "Epoch 2/10\n",
      "4651/6870 [===================>..........] - ETA: 24:18 - loss: 1.2574 - accuracy: 0.6104"
     ]
    }
   ],
   "source": [
    "# fit our model\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cWnH8xDFT53P"
   },
   "outputs": [],
   "source": [
    "# gets the latest checkpoint file\n",
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0x6ERwGRUPW7"
   },
   "outputs": [],
   "source": [
    "# builds models based on saved wieghts in checkpoint directory\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### these model checkpoints will now be used in our frontend app to generate song lyrics based on genre"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "text_gen.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
